As a Data Science minor at UC Berkeley, I learned industry-standard machine learning techniques while considering how real-world models can produce—and reinforce—societal harm. In the project linked above, I analyzed a predictive model developed by the Chicago Cook County Assessor’s Office (CCAO) to estimate residential property values for tax assessment. Prior research had shown that this model contributed to a highly regressive tax system, disproportionately over-taxing African-American and Latinx homeowners across Cook County.

My task was to develop an alternative model to predict housing market values using machine learning regression techniques applied to the existing CCAO dataset. I conducted extensive exploratory data analysis, using visualization methods and correlation metrics to identify useful variables and design tailored statistical treatment for each feature. From there, I designed and implemented a feature engineering pipeline, including custom functions for individual features, to improve both predictive performance and robustness. Although this approach was more technically demanding and time-intensive, it resulted in a model with strong accuracy across both training and test sets.

This project is especially relevant to Anthropic’s Societal Impacts work because it required interrogating not just model performance, but who a model serves and who it harms. It reinforced for me that high accuracy alone is not a sufficient metric for responsible AI—and that careful evaluation, feature choices, and assumptions can materially shape real-world outcomes. That mindset directly aligns with Anthropic’s approach to understanding AI as a sociotechnical system, and with the team’s focus on using rigorous empirical analysis to inform more equitable, trustworthy model behavior.


(Note: I received an A grade on this project. There was a glitch in this assignemnt, but all test cases were passed)
